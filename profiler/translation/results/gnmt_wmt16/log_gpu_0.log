2021-09-26 05:58:15 - INFO - 0 - Saving results to: results/gnmt_wmt16
2021-09-26 05:58:15 - INFO - 0 - Run arguments: Namespace(apex_message_size=10000000.0, arch='gnmt_large', batch_size=64, beam_size=5, bucketing=True, cov_penalty_factor=0.1, cuda=True, cudnn=True, dataset_dir='/data/DNN_Dataset/wmt', disable_eval=False, enable_apex_allreduce_overlap=False, epochs=20, grad_clip=5.0, intra_epoch_eval=0, keep_checkpoints=0, len_norm_const=5.0, len_norm_factor=0.6, local_rank=0, math='fp32', max_length_test=150, max_length_train=50, max_length_val=150, max_size=None, min_length_test=0, min_length_train=0, min_length_val=0, model_config="{'num_layers': 4, 'hidden_size': 1024, 'dropout':0.2, 'share_embedding': False}", optimization_config="{'optimizer': 'Adam', 'lr': 5e-4, 'betas':(0.9,0.999)}", print_freq=10, rank=0, results_dir='results', resume=None, save='gnmt_wmt16', save_all=False, save_freq=5000, save_path='results/gnmt_wmt16', scheduler_config="{'lr_method':'none', 'warmup_iters':0, 'remain_steps':0, 'decay_steps':0}", seed=None, smoothing=0.1, start_epoch=0, target_bleu=21.8, test_batch_size=128, test_loader_workers=0, train_loader_workers=2, val_batch_size=64, val_loader_workers=0)
2021-09-26 06:00:22 - INFO - 0 - L2 promotion: 128B
2021-09-26 06:00:22 - INFO - 0 - Using random master seed: 3772940116
2021-09-26 06:00:22 - INFO - 0 - Worker 0 is using worker seed: 3266357905
2021-09-26 06:00:22 - INFO - 0 - Building vocabulary from /data/DNN_Dataset/wmt/vocab.bpe.32000
2021-09-26 06:00:23 - INFO - 0 - Size of vocabulary: 32320
2021-09-26 06:00:23 - INFO - 0 - Processing data from /data/DNN_Dataset/wmt/train.tok.clean.bpe.32000.en
2021-09-26 06:02:35 - INFO - 0 - Processing data from /data/DNN_Dataset/wmt/train.tok.clean.bpe.32000.de
2021-09-26 06:05:07 - INFO - 0 - Filtering data, min len: 0, max len: 50
2021-09-26 06:05:13 - INFO - 0 - Pairs before: 4068191, after: 3498161
2021-09-26 06:05:14 - INFO - 0 - Processing data from /data/DNN_Dataset/wmt/newstest_dev.tok.clean.bpe.32000.en
2021-09-26 06:05:14 - INFO - 0 - Processing data from /data/DNN_Dataset/wmt/newstest_dev.tok.clean.bpe.32000.de
2021-09-26 06:05:15 - INFO - 0 - Filtering data, min len: 0, max len: 150
2021-09-26 06:05:15 - INFO - 0 - Pairs before: 5100, after: 5100
2021-09-26 06:05:16 - INFO - 0 - Processing data from /data/DNN_Dataset/wmt/newstest2014.tok.bpe.32000.en
2021-09-26 06:05:16 - INFO - 0 - Filtering data, min len: 0, max len: 150
2021-09-26 06:05:16 - INFO - 0 - Pairs before: 3003, after: 3003
2021-09-26 06:05:19 - INFO - 0 - GNMT(
  (encoder): ResidualRecurrentEncoder(
    (rnn_layers): ModuleList(
      (0): EmuBidirLSTM(
        (bidir): LSTM(1024, 1024, bidirectional=True)
        (layer1): LSTM(1024, 1024)
        (layer2): LSTM(1024, 1024)
      )
      (1): LSTM(2048, 1024)
      (2): LSTM(1024, 1024)
      (3): LSTM(1024, 1024)
    )
    (dropout): Dropout(p=0.2)
    (embedder): Embedding(32320, 1024, padding_idx=0)
  )
  (decoder): ResidualRecurrentDecoder(
    (att_rnn): RecurrentAttention(
      (rnn): LSTM(1024, 1024)
      (attn): BahdanauAttention(
        (linear_q): Linear(in_features=1024, out_features=1024, bias=False)
        (linear_k): Linear(in_features=1024, out_features=1024, bias=False)
        (dropout): Dropout(p=0)
      )
      (dropout): Dropout(p=0)
    )
    (rnn_layers): ModuleList(
      (0): LSTM(2048, 1024)
      (1): LSTM(2048, 1024)
      (2): LSTM(2048, 1024)
    )
    (embedder): Embedding(32320, 1024, padding_idx=0)
    (classifier): Classifier(
      (classifier): Linear(in_features=1024, out_features=32320, bias=True)
    )
    (dropout): Dropout(p=0.2)
  )
)
2021-09-26 06:05:19 - INFO - 0 - Building LabelSmoothingLoss (smoothing: 0.1)
2021-09-26 06:05:19 - INFO - 0 - Training optimizer: {'optimizer': 'Adam', 'lr': 0.0005, 'betas': (0.9, 0.999)}
2021-09-26 06:05:19 - INFO - 0 - Training LR Schedule: {'lr_method': 'none', 'warmup_iters': 0, 'remain_steps': 0, 'decay_steps': 0}
2021-09-26 06:05:19 - INFO - 0 - Number of parameters: 193766977
2021-09-26 06:05:19 - INFO - 0 - Saving state of the tokenizer
2021-09-26 06:06:21 - INFO - 0 - Initializing fp32 optimizer
2021-09-26 06:06:21 - INFO - 0 - Using optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    initial_lr: 0.0005
    lr: 0.0005
    weight_decay: 0
)
2021-09-26 06:06:21 - INFO - 0 - Starting epoch 0
2021-09-26 06:06:40 - INFO - 0 - Sampler for epoch 0 uses seed 1853627798
2021-09-26 06:06:41 - INFO - 0 - Sampler for epoch 0 uses seed 1853627798
2021-09-26 06:06:41 - INFO - 0 - TRAIN [0][0/54658]	Time 0.740 (0.000)	Data 0.63109 (0.00000)	Tok/s 2313 (0)	Loss/tok 10.3844 (0.0000)	Learning Rate [0.0005]
2021-09-26 06:06:43 - INFO - 0 - TRAIN [0][10/54658]	Time 0.212 (0.191)	Data 0.00151 (0.00131)	Tok/s 17616 (16801)	Loss/tok 8.4623 (9.6993)	Learning Rate [0.0005]
2021-09-26 06:06:45 - INFO - 0 - TRAIN [0][20/54658]	Time 0.124 (0.179)	Data 0.00145 (0.00133)	Tok/s 15498 (16115)	Loss/tok 7.9037 (9.0392)	Learning Rate [0.0005]
2021-09-26 06:06:47 - INFO - 0 - TRAIN [0][30/54658]	Time 0.267 (0.193)	Data 0.00148 (0.00134)	Tok/s 16482 (16399)	Loss/tok 8.2394 (8.6950)	Learning Rate [0.0005]
2021-09-26 06:06:49 - INFO - 0 - TRAIN [0][40/54658]	Time 0.137 (0.183)	Data 0.00143 (0.00135)	Tok/s 16790 (16183)	Loss/tok 7.9076 (8.5523)	Learning Rate [0.0005]
2021-09-26 06:06:51 - INFO - 0 - TRAIN [0][50/54658]	Time 0.263 (0.185)	Data 0.00145 (0.00135)	Tok/s 19047 (16547)	Loss/tok 8.0400 (8.4084)	Learning Rate [0.0005]
2021-09-26 06:06:53 - INFO - 0 - TRAIN [0][60/54658]	Time 0.264 (0.189)	Data 0.00149 (0.00134)	Tok/s 20442 (16653)	Loss/tok 7.9173 (8.3115)	Learning Rate [0.0005]
2021-09-26 06:06:55 - INFO - 0 - TRAIN [0][70/54658]	Time 0.255 (0.189)	Data 0.00145 (0.00134)	Tok/s 15566 (16699)	Loss/tok 7.8618 (8.2304)	Learning Rate [0.0005]
2021-09-26 06:06:56 - INFO - 0 - TRAIN [0][80/54658]	Time 0.209 (0.185)	Data 0.00145 (0.00134)	Tok/s 19060 (16719)	Loss/tok 7.8198 (8.1727)	Learning Rate [0.0005]
2021-09-26 06:06:58 - INFO - 0 - TRAIN [0][90/54658]	Time 0.249 (0.185)	Data 0.00144 (0.00134)	Tok/s 15279 (16623)	Loss/tok 7.8224 (8.1140)	Learning Rate [0.0005]
2021-09-26 06:07:00 - INFO - 0 - TRAIN [0][100/54658]	Time 0.244 (0.187)	Data 0.00150 (0.00134)	Tok/s 15071 (16596)	Loss/tok 7.6926 (8.0658)	Learning Rate [0.0005]
