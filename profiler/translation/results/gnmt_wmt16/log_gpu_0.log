2021-09-10 01:51:43 - INFO - 0 - Saving results to: results/gnmt_wmt16
2021-09-10 01:51:43 - INFO - 0 - Run arguments: Namespace(apex_message_size=10000000.0, arch='gnmt_large', batch_size=64, beam_size=5, bucketing=True, cov_penalty_factor=0.1, cuda=True, cudnn=True, dataset_dir='/data/DNN_Dataset/wmt', disable_eval=False, enable_apex_allreduce_overlap=False, epochs=20, grad_clip=5.0, intra_epoch_eval=0, keep_checkpoints=0, len_norm_const=5.0, len_norm_factor=0.6, local_rank=0, math='fp32', max_length_test=150, max_length_train=50, max_length_val=150, max_size=None, min_length_test=0, min_length_train=0, min_length_val=0, model_config="{'num_layers': 4, 'hidden_size': 1024, 'dropout':0.2, 'share_embedding': False}", optimization_config="{'optimizer': 'Adam', 'lr': 5e-4, 'betas':(0.9,0.999)}", print_freq=10, rank=0, results_dir='results', resume=None, save='gnmt_wmt16', save_all=False, save_freq=5000, save_path='results/gnmt_wmt16', scheduler_config="{'lr_method':'none', 'warmup_iters':0, 'remain_steps':0, 'decay_steps':0}", seed=None, smoothing=0.1, start_epoch=0, target_bleu=21.8, test_batch_size=128, test_loader_workers=0, train_loader_workers=2, val_batch_size=64, val_loader_workers=0)
2021-09-10 01:53:54 - INFO - 0 - L2 promotion: 128B
2021-09-10 01:53:54 - INFO - 0 - Using random master seed: 2134266024
2021-09-10 01:53:54 - INFO - 0 - Worker 0 is using worker seed: 347050045
2021-09-10 01:53:54 - INFO - 0 - Building vocabulary from /data/DNN_Dataset/wmt/vocab.bpe.32000
2021-09-10 01:53:55 - INFO - 0 - Size of vocabulary: 32320
2021-09-10 01:53:55 - INFO - 0 - Processing data from /data/DNN_Dataset/wmt/train.tok.clean.bpe.32000.en
2021-09-10 01:56:08 - INFO - 0 - Processing data from /data/DNN_Dataset/wmt/train.tok.clean.bpe.32000.de
2021-09-10 01:58:42 - INFO - 0 - Filtering data, min len: 0, max len: 50
2021-09-10 01:58:48 - INFO - 0 - Pairs before: 4068191, after: 3498161
2021-09-10 01:58:49 - INFO - 0 - Processing data from /data/DNN_Dataset/wmt/newstest_dev.tok.clean.bpe.32000.en
2021-09-10 01:58:49 - INFO - 0 - Processing data from /data/DNN_Dataset/wmt/newstest_dev.tok.clean.bpe.32000.de
2021-09-10 01:58:50 - INFO - 0 - Filtering data, min len: 0, max len: 150
2021-09-10 01:58:50 - INFO - 0 - Pairs before: 5100, after: 5100
2021-09-10 01:58:51 - INFO - 0 - Processing data from /data/DNN_Dataset/wmt/newstest2014.tok.bpe.32000.en
2021-09-10 01:58:51 - INFO - 0 - Filtering data, min len: 0, max len: 150
2021-09-10 01:58:51 - INFO - 0 - Pairs before: 3003, after: 3003
2021-09-10 01:58:53 - INFO - 0 - GNMT(
  (encoder): ResidualRecurrentEncoder(
    (rnn_layers): ModuleList(
      (0): EmuBidirLSTM(
        (bidir): LSTM(1024, 1024, bidirectional=True)
        (layer1): LSTM(1024, 1024)
        (layer2): LSTM(1024, 1024)
      )
      (1): LSTM(2048, 1024)
      (2): LSTM(1024, 1024)
      (3): LSTM(1024, 1024)
    )
    (dropout): Dropout(p=0.2)
    (embedder): Embedding(32320, 1024, padding_idx=0)
  )
  (decoder): ResidualRecurrentDecoder(
    (att_rnn): RecurrentAttention(
      (rnn): LSTM(1024, 1024)
      (attn): BahdanauAttention(
        (linear_q): Linear(in_features=1024, out_features=1024, bias=False)
        (linear_k): Linear(in_features=1024, out_features=1024, bias=False)
        (dropout): Dropout(p=0)
      )
      (dropout): Dropout(p=0)
    )
    (rnn_layers): ModuleList(
      (0): LSTM(2048, 1024)
      (1): LSTM(2048, 1024)
      (2): LSTM(2048, 1024)
    )
    (embedder): Embedding(32320, 1024, padding_idx=0)
    (classifier): Classifier(
      (classifier): Linear(in_features=1024, out_features=32320, bias=True)
    )
    (dropout): Dropout(p=0.2)
  )
)
2021-09-10 01:58:53 - INFO - 0 - Building LabelSmoothingLoss (smoothing: 0.1)
2021-09-10 01:58:53 - INFO - 0 - Training optimizer: {'optimizer': 'Adam', 'lr': 0.0005, 'betas': (0.9, 0.999)}
2021-09-10 01:58:54 - INFO - 0 - Training LR Schedule: {'lr_method': 'none', 'warmup_iters': 0, 'remain_steps': 0, 'decay_steps': 0}
2021-09-10 01:58:54 - INFO - 0 - Number of parameters: 193766977
2021-09-10 01:58:54 - INFO - 0 - Saving state of the tokenizer
2021-09-10 01:59:55 - INFO - 0 - Initializing fp32 optimizer
2021-09-10 01:59:55 - INFO - 0 - Using optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    initial_lr: 0.0005
    lr: 0.0005
    weight_decay: 0
)
2021-09-10 01:59:55 - INFO - 0 - Starting epoch 0
2021-09-10 02:00:14 - INFO - 0 - Sampler for epoch 0 uses seed 3984719093
2021-09-10 02:00:15 - INFO - 0 - Sampler for epoch 0 uses seed 3984719093
2021-09-10 02:00:16 - INFO - 0 - TRAIN [0][0/54658]	Time 0.893 (0.000)	Data 0.65118 (0.00000)	Tok/s 4830 (0)	Loss/tok 10.3792 (0.0000)	Learning Rate [0.0005]
2021-09-10 02:00:18 - INFO - 0 - TRAIN [0][10/54658]	Time 0.099 (0.205)	Data 0.00122 (0.00113)	Tok/s 14652 (16829)	Loss/tok 7.9692 (9.4595)	Learning Rate [0.0005]
2021-09-10 02:00:20 - INFO - 0 - TRAIN [0][20/54658]	Time 0.134 (0.196)	Data 0.00125 (0.00120)	Tok/s 16340 (16894)	Loss/tok 7.9765 (8.8824)	Learning Rate [0.0005]
2021-09-10 02:00:22 - INFO - 0 - TRAIN [0][30/54658]	Time 0.204 (0.191)	Data 0.00125 (0.00122)	Tok/s 17848 (16927)	Loss/tok 8.0146 (8.6208)	Learning Rate [0.0005]
2021-09-10 02:00:23 - INFO - 0 - TRAIN [0][40/54658]	Time 0.202 (0.189)	Data 0.00128 (0.00123)	Tok/s 16695 (16967)	Loss/tok 8.0144 (8.4620)	Learning Rate [0.0005]
2021-09-10 02:00:25 - INFO - 0 - TRAIN [0][50/54658]	Time 0.270 (0.192)	Data 0.00125 (0.00124)	Tok/s 20733 (17082)	Loss/tok 7.9922 (8.3490)	Learning Rate [0.0005]
2021-09-10 02:00:27 - INFO - 0 - TRAIN [0][60/54658]	Time 0.262 (0.193)	Data 0.00126 (0.00124)	Tok/s 19704 (17165)	Loss/tok 8.0288 (8.2715)	Learning Rate [0.0005]
2021-09-10 02:00:29 - INFO - 0 - TRAIN [0][70/54658]	Time 0.248 (0.191)	Data 0.00126 (0.00124)	Tok/s 16742 (17088)	Loss/tok 7.9384 (8.2127)	Learning Rate [0.0005]
2021-09-10 02:00:31 - INFO - 0 - TRAIN [0][80/54658]	Time 0.269 (0.191)	Data 0.00131 (0.00125)	Tok/s 19756 (17084)	Loss/tok 7.8506 (8.1491)	Learning Rate [0.0005]
2021-09-10 02:00:33 - INFO - 0 - TRAIN [0][90/54658]	Time 0.118 (0.191)	Data 0.00127 (0.00125)	Tok/s 15822 (17034)	Loss/tok 7.3248 (8.0978)	Learning Rate [0.0005]
2021-09-10 02:00:35 - INFO - 0 - TRAIN [0][100/54658]	Time 0.109 (0.189)	Data 0.00125 (0.00125)	Tok/s 15540 (16947)	Loss/tok 7.2650 (8.0508)	Learning Rate [0.0005]
